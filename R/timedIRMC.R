#' Fit aggregate data using Iterative Reweighting with Monte Carlo updates
#'
#' @description
#' `timedIRMC` implements the Iterative Reweighting (IRMC) algorithm for parameter estimation of
#' aggregate data models, iterating over maximum likelihood updates with weighted Monte Carlo
#' updates. This function is used to compare the performance of different implementations of
#' aggregate data modeling.
#'
#' @param init Initial parameter values for optimization. These should be transformed parameters
#'             as generated by `opts$pt`.
#' @param opts A list of model options generated by `genopts()`. Contains settings for the model,
#'             including the prediction function, time points, parameter structure, and simulation
#'             settings.
#' @param obs Observed data in aggregate form (mean and covariance) or as a matrix of raw data.
#' @param maxiter Maximum number of iterations for the optimization algorithm. Default is 100.
#' @param convcrit_nll Convergence criterion for the negative log-likelihood. The algorithm stops
#'                     when the relative change in negative log-likelihood is less than this value.
#'                     Default is 5e-04.
#' @param nomap Logical indicating whether to use multiple models (FALSE) or a single model (TRUE).
#'              Default is TRUE.
#'
#' @returns A data frame containing:
#' \itemize{
#'   \item `p`: List of parameter estimates for each iteration
#'   \item `nll`: Negative log-likelihood values
#'   \item `time`: Computation time for each iteration
#'   \item `iter`: Iteration number
#' }
#'
#' @details
#' The function uses the Iterative Reweighting algorithm with Monte Carlo sampling for optimization.
#' At each iteration, it generates Monte Carlo samples and updates the parameter estimates using
#' weighted importance sampling. The algorithm continues until convergence or until the maximum
#' number of iterations is reached.
#'
#' @examples
#' # Load required libraries
#' library(admr)
#' library(rxode2)
#' library(nlmixr2)
#' library(dplyr)
#' library(tidyr)
#' library(mnorm)
#' 
#' # Load and prepare data
#' data(examplomycin)
#' examplomycin_wide <- examplomycin %>%
#'   filter(EVID != 101) %>%
#'   dplyr::select(ID, TIME, DV) %>%
#'   pivot_wider(names_from = TIME, values_from = DV) %>%
#'   dplyr::select(-c(1))
#' 
#' # Create aggregated data
#' examplomycin_aggregated <- examplomycin_wide %>%
#'   admr::meancov()
#' 
#' # Define RxODE model
#' rxModel <- RxODE({
#'   cp = linCmt(
#'     cl,           # Clearance
#'     v1,           # Volume of the central compartment
#'     v2,           # Volume of the peripheral compartment
#'     q,            # Inter-compartmental clearance
#'     ka            # Absorption rate constant
#'   )
#' })
#' 
#' # Define prediction function
#' predder <- function(time, theta_i, dose = 100) {
#'   n_individuals <- nrow(theta_i)
#'   if (is.null(n_individuals)) n_individuals <- 1
#'   
#'   ev <- eventTable(amount.units="mg", time.units="hours")
#'   ev$add.dosing(dose = dose, nbr.doses = 1, start.time = 0)
#'   ev$add.sampling(time)
#'   
#'   out <- rxSolve(rxModel, params = theta_i, events = ev, cores = 0)
#'   cp_matrix <- matrix(out$cp, nrow = n_individuals, ncol = length(time),
#'                       byrow = TRUE)
#'   return(cp_matrix)
#' }
#' 
#' # Create options
#' opts <- genopts(
#'   time = c(.1, .25, .5, 1, 2, 3, 5, 8, 12),
#'   p = list(
#'     beta = c(cl = 5, v1 = 10, v2 = 30, q = 10, ka = 1),
#'     Omega = matrix(c(0.09, 0, 0, 0, 0,
#'                      0, 0.09, 0, 0, 0,
#'                      0, 0, 0.09, 0, 0,
#'                      0, 0, 0, 0.09, 0,
#'                      0, 0, 0, 0, 0.09), nrow = 5, ncol = 5),
#'     Sigma_prop = 0.04
#'   ),
#'   nsim = 2500,
#'   n = 500,
#'   fo_appr = FALSE,
#'   omega_expansion = 1.2,
#'   f = predder
#' )
#' 
#' # Run optimization
#' result <- timedIRMC(opts$pt, opts, examplomycin_aggregated)
#' print(result)
#'
#' @export
timedIRMC <- function(init, opts, obs, maxiter = 100, convcrit_nll = 5e-04, nomap = TRUE) {
  # Convert initial parameters and observed data to optimization format
  if (nomap) {
    opts <- opts %>% p2opts(init) %>% obs2opts(obs)
  } else {
    opts <- map(seq_along(opts),function(i) opts[[i]] %>% p2opts(init) %>% obs2opts(obs[[i]]))
  }

  # Initialize results data frame to store iteration history
  res <- tibble(p=vector("list",maxiter),
                nll=NA,
                appr_nll=NA,
                time=Sys.time(),
                iter=1)
  
  # Store initial values and compute initial negative log-likelihood
  res$p[[1]] <- init
  res$time[1] <- Sys.time()
  res$nll[1] <- compute_nll(opts, init, nomap)
  res$appr_nll[1] <- res$nll[1]
  message(paste0("iteration ",1,", nll=",res$nll[1]))
  
  # Initialize p-values for convergence checking
  pvals <- rep(0,length(init)+1)
  
  # Main optimization loop
  for (i in 2:maxiter) {
    # Generate objective function for optimization
    if (nomap) {
      ff <- maxfunc(p2opts(opts,init))
    } else {
      ffs <- map(opts,~maxfunc(p2opts(.,init)))
      ff <- function(p) Reduce('+',map(ffs,~.(p)))
    }

    # Create wrapper function for nloptr
    ff_nloptr <- function(params) {
      ff(params)
    }

    # Run BOBYQA optimization with bounds
    m0 <- nloptr::nloptr(
      x0 = init,
      eval_f = ff_nloptr,
      lb = init - 2,  # Lower bounds: 2 units below initial values
      ub = init + 2,  # Upper bounds: 2 units above initial values
      opts = list(
        algorithm = "NLOPT_LN_BOBYQA",
        ftol_rel=.Machine$double.eps^2,  # Relative function tolerance
        maxeval = 5000  # Maximum number of function evaluations
      )
    )

    # Update parameters and store results
    init <- m0$solution
    res$p[[i]] <- init
    res$time[i] <- Sys.time()
    res$nll[i] <- compute_nll(opts, init, nomap)
    res$appr_nll[i] <- m0$objective
    res$iter[i] <- i
    message(paste0("iteration ",i,", nll=",res$nll[i]))
    
    # Check for parameter stationarity after 10 iterations
    if (i>10) {
      # Extract last 10 iterations of parameters and objective function
      pset <- do.call(rbind,res$p) %>% cbind(res$nll[!is.na(res$nll)]) %>% tail(10)
      # Compute p-values for linear trend in each parameter
      pvals <- map(1:ncol(pset),function(colN) {
        xx <- 1:10
        yy <- pset[,colN]
        summary(lm(yy~xx))$coef[2,4]
      })
    }
    
    # Check convergence criteria
    if (all(pvals>0.05)) {message("should break now due to stationary ofv+parameters");break()}
    if (abs(res$nll[i]-res$appr_nll[i]) <convcrit_nll) {message("should break now due to no difference between OFV and appr OFV");break()}
  }
  
  # Return results for successful iterations
  res[!is.na(res$nll),]
}

